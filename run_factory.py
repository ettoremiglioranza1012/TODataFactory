#!/usr/bin/env python3
"""
run_factory.py - Dataset Generation Pipeline for ML Training

This script generates distributed load cases and runs the TopOpt solver
to create training data for 3D U-Net topology optimization surrogates.
"""

import subprocess
import numpy as np
import os
import tempfile
from pathlib import Path


# Stencil sizes used for padding in the C++ code (from definitions.h)
STENCIL_SIZE_X = 1  # must be 1
STENCIL_SIZE_Y = 8  # set to 4 for AVX2, or 8 for AVX512
STENCIL_SIZE_Z = 1  # must be 1


def compute_wrap_dimensions(nelx: int, nely: int, nelz: int) -> tuple:
    """
    Compute the wrapped grid dimensions used by the C++ solver.
    
    The C++ code adds padding for SIMD alignment and halo cells.
    This must match the logic in grid_utilities.c:initializeGridContext()
    
    Args:
        nelx, nely, nelz: Number of elements in each direction
    
    Returns:
        tuple: (wrapx, wrapy, wrapz) - padded grid dimensions
    """
    # Padding for SIMD alignment
    paddingx = (STENCIL_SIZE_X - ((nelx + 1) % STENCIL_SIZE_X)) % STENCIL_SIZE_X
    paddingy = (STENCIL_SIZE_Y - ((nely + 1) % STENCIL_SIZE_Y)) % STENCIL_SIZE_Y
    paddingz = (STENCIL_SIZE_Z - ((nelz + 1) % STENCIL_SIZE_Z)) % STENCIL_SIZE_Z
    
    # Wrap dimensions include padding + 3 halo cells
    wrapx = nelx + paddingx + 3
    wrapy = nely + paddingy + 3
    wrapz = nelz + paddingz + 3
    
    return wrapx, wrapy, wrapz


def compute_ndof(nelx: int, nely: int, nelz: int) -> int:
    """
    Compute the total number of degrees of freedom for the wrapped grid.
    
    Args:
        nelx, nely, nelz: Number of elements in each direction
    
    Returns:
        int: Total DOFs (3 * wrapx * wrapy * wrapz)
    """
    wrapx, wrapy, wrapz = compute_wrap_dimensions(nelx, nely, nelz)
    return 3 * wrapx * wrapy * wrapz


def node_index_to_flat(i: int, j: int, k: int, wrapy: int, wrapz: int) -> int:
    """
    Convert (i, j, k) node coordinates to flat index.
    
    Uses the C++ indexing scheme: nidx = i * wrapy * wrapz + wrapy * k + j
    
    Args:
        i: x-coordinate in wrapped grid (1-indexed in C++, we handle offset)
        j: y-coordinate in wrapped grid
        k: z-coordinate in wrapped grid
        wrapy, wrapz: Wrapped grid dimensions
    
    Returns:
        int: Flat node index
    """
    return i * wrapy * wrapz + wrapy * k + j


def read_density_field(nelx: int, nely: int, nelz: int, filename: str) -> np.ndarray:
    """
    Read binary density field generated by C++ solver.
    
    Args:
        nelx, nely, nelz: Number of elements in each direction
        filename: Path to binary density file
    
    Returns:
        numpy array: Density field shaped (nelx, nely, nelz)
    """
    expected_size = nelx * nely * nelz
    
    # Read binary file
    density_flat = np.fromfile(filename, dtype=np.float64)
    
    if density_flat.size != expected_size:
        raise ValueError(
            f"Density file size mismatch. Expected {expected_size} values, "
            f"got {density_flat.size}"
        )
    
    # Reshape to match C++ indexing: i1 * nely * nelz + k1 * nely + j1
    # This gives us (nelx, nelz, nely) in C order
    density = density_flat.reshape((nelx, nelz, nely))
    
    # Transpose to standard (nelx, nely, nelz) format
    density = np.transpose(density, (0, 2, 1))
    
    return density.astype(np.float32)


def generate_random_load(
    nelx: int,
    nely: int,
    nelz: int,
    filename: str,
    total_force: float = -1000.0,
    random_seed: int = None
) -> tuple:
    """
    Generate a randomized distributed load case for ML training.
    
    Creates a random off-center load patch on the top face and generates
    ML input tensors for 3D U-Net training.
    
    Args:
        nelx, nely, nelz: Number of elements in each direction
        filename: Output filename for the binary force vector
        total_force: Total force magnitude (negative = downward in z)
        random_seed: Random seed for reproducibility (None = random)
    
    Returns:
        tuple: (binary_filename, input_tensor, metadata_dict)
            - binary_filename: Path to the binary force file
            - input_tensor: numpy array shape (nelx, nely, nelz, 4) channels-last
                Channel 0: Solid domain mask (1.0 for solid, 0.0 for void)
                Channel 1: Force X component
                Channel 2: Force Y component
                Channel 3: Force Z component
            - metadata_dict: Load case metadata including load_center
    """
    # Set random seed if provided
    if random_seed is not None:
        np.random.seed(random_seed)
    
    # Compute wrapped grid dimensions for C++ solver
    wrapx, wrapy, wrapz = compute_wrap_dimensions(nelx, nely, nelz)
    ndof = 3 * wrapx * wrapy * wrapz
    
    # Initialize force vector for C++ solver (sparse, wrapped grid)
    F_solver = np.zeros(ndof, dtype=np.float64)
    
    # Number of nodes in each direction (non-wrapped, for visualization)
    nx_nodes = nelx + 1
    ny_nodes = nely + 1
    nz_nodes = nelz + 1
    
    # Randomize load patch center (20% to 80% of domain to avoid edges)
    cx_normalized = np.random.uniform(0.2, 0.8)
    cy_normalized = np.random.uniform(0.2, 0.8)
    
    cx = int(cx_normalized * nelx)  # Element-space center x
    cy = int(cy_normalized * nely)  # Element-space center y
    
    # Randomize patch radius (3-5 elements)
    radius = np.random.uniform(3.0, 5.0)
    radius2 = radius * radius
    
    # Top face is at z = nelz (last layer of nodes in 0-indexed space)
    k = nz_nodes - 1
    
    # Count nodes in the load region (circular patch)
    load_nodes = []
    for x in range(nx_nodes):
        for y in range(ny_nodes):
            dx = x - cx
            dy = y - cy
            dist2 = dx * dx + dy * dy
            
            if dist2 <= radius2:
                # Convert to wrapped grid coordinates (add 1 for halo offset)
                i = x + 1
                j = y + 1
                kk = k + 1
                
                nidx = node_index_to_flat(i, j, kk, wrapy, wrapz)
                load_nodes.append((nidx, x, y))
    
    num_load_nodes = len(load_nodes)
    if num_load_nodes == 0:
        raise ValueError(f"No nodes in load region! Center: ({cx}, {cy}), Radius: {radius}")
    
    # Distribute the total force evenly across load nodes
    nodal_force = total_force / num_load_nodes
    
    # Apply force in z-direction to solver force vector
    for nidx, _, _ in load_nodes:
        dof_z = 3 * nidx + 2  # z-component
        F_solver[dof_z] = nodal_force
    
    # Save binary file for C++ solver
    F_solver.tofile(filename)
    
    # ========================================
    # Generate ML Input Tensor (Channels Last)
    # ========================================
    # Shape: (nelx, nely, nelz, 4)
    # Channel 0: Solid domain mask
    # Channel 1-3: Force components (Fx, Fy, Fz)
    
    input_tensor = np.zeros((nelx, nely, nelz, 4), dtype=np.float32)
    
    # Channel 0: Solid domain mask (all 1s for a solid block)
    # In the future, this could represent variable geometry
    input_tensor[:, :, :, 0] = 1.0
    
    # Channels 1-3: Force components
    # Map forces from node space to element centers for visualization
    # For each element, check if any of its 4 top-face nodes has a load
    for nidx, node_x, node_y in load_nodes:
        # Each node is shared by up to 4 elements
        # Map to elements that have this node
        for elem_x in [node_x - 1, node_x]:
            for elem_y in [node_y - 1, node_y]:
                # Check bounds
                if 0 <= elem_x < nelx and 0 <= elem_y < nely:
                    # Top element (z = nelz - 1)
                    elem_z = nelz - 1
                    
                    # Add force contribution (divide by 4 since node is shared)
                    # Fx (Channel 1) = 0
                    # Fy (Channel 2) = 0
                    # Fz (Channel 3) = nodal_force / 4
                    input_tensor[elem_x, elem_y, elem_z, 3] += nodal_force / 4.0
    
    # Metadata
    metadata = {
        "nelx": nelx,
        "nely": nely,
        "nelz": nelz,
        "wrapx": wrapx,
        "wrapy": wrapy,
        "wrapz": wrapz,
        "ndof": ndof,
        "file_size_bytes": ndof * 8,
        "num_load_nodes": num_load_nodes,
        "nodal_force": nodal_force,
        "total_force": total_force,
        "load_center": [cx, cy],
        "load_radius": radius,
        "patch_center_normalized": [cx_normalized, cy_normalized],
        "filename": filename,
        "tensor_shape": input_tensor.shape,
        "random_seed": random_seed
    }
    
    return (filename, input_tensor, metadata)


def run_solver(
    nelx: int,
    nely: int,
    nelz: int,
    vol_frac: float,
    penal: float = 3.0,
    rmin: float = 1.5,
    iters: int = 20,
    total_force: float = -1000.0,
    random_seed: int = None,
    keep_load_file: bool = False,
    save_input_tensor: bool = False,
    output_dir: str = None
) -> dict:
    """
    Generate a randomized load case and run the TopOpt solver.
    
    Args:
        nelx, nely, nelz: Number of elements in each direction
        vol_frac: Volume fraction constraint
        penal: SIMP penalization factor
        rmin: Filter radius
        iters: Number of design iterations
        total_force: Total applied force
        random_seed: Random seed for reproducibility
        keep_load_file: If True, don't delete the temporary load file
        save_input_tensor: If True, save the input tensor to disk
        output_dir: Directory to save input tensor (required if save_input_tensor=True)
    
    Returns:
        dict: Solver results, input tensor, and metadata
    """
    # Path to the solver executable
    script_dir = Path(__file__).parent.resolve()
    exe_path = script_dir / "TopOpt-in-OpenMP" / "top3d"
    
    if not exe_path.exists():
        raise FileNotFoundError(f"Solver not found at: {exe_path}")
    
    # Create temporary load file
    fd, load_file = tempfile.mkstemp(suffix=".bin", prefix="topopt_load_")
    os.close(fd)
    
    try:
        # Generate randomized load case with ML input tensor
        print(f"üè≠ Factory: Generating randomized load case...")
        _, input_tensor, load_meta = generate_random_load(
            nelx, nely, nelz, load_file, total_force, random_seed
        )
        
        print(f"   Load center: ({load_meta['load_center'][0]}, {load_meta['load_center'][1]})")
        print(f"   Patch radius: {load_meta['load_radius']:.2f} elements")
        print(f"   Load applied to {load_meta['num_load_nodes']} nodes")
        print(f"   Force per node: {load_meta['nodal_force']:.4f} N")
        print(f"   Input tensor shape: {input_tensor.shape}")
        
        # Save input tensor if requested
        tensor_path = None
        if save_input_tensor:
            if output_dir is None:
                raise ValueError("output_dir must be specified when save_input_tensor=True")
            
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # Generate unique filename
            seed_str = f"_seed{random_seed}" if random_seed is not None else ""
            tensor_filename = f"input_X_{nelx}x{nely}x{nelz}{seed_str}.npy"
            tensor_path = output_path / tensor_filename
            
            np.save(tensor_path, input_tensor)
            print(f"   Saved input tensor: {tensor_path}")
        
        # Build command for C++ solver
        # Use default nl=4 to match C++ default and avoid multigrid bugs
        nl = 4
        size_incr = 2 ** (nl - 1)  # = 8
        
        nelx_coarse = nelx // size_incr
        nely_coarse = nely // size_incr  
        nelz_coarse = nelz // size_incr
        
        if nelx_coarse * size_incr != nelx:
            print(f"‚ö†Ô∏è  Warning: nelx={nelx} not divisible by {size_incr}, using {nelx_coarse * size_incr}")
        
        cmd = [
            str(exe_path),
            "-x", str(nelx_coarse),
            "-y", str(nely_coarse),
            "-z", str(nelz_coarse),
            "-v", str(vol_frac),
            "-r", str(rmin),
            "-i", str(iters),
            "-l", str(nl),
            "-f", load_file
        ]
        
        print(f"\nüîß Running: {' '.join(cmd)}")
        print("-" * 60)
        
        # Execute solver
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=str(script_dir / "TopOpt-in-OpenMP")
        )
        
        # Print output
        print(result.stdout)
        
        if result.returncode != 0:
            print(f"‚ùå Solver failed with exit code {result.returncode}")
            print(result.stderr)
            return {
                "success": False,
                "error": result.stderr,
                "returncode": result.returncode,
                "input_tensor": input_tensor,
                "load_metadata": load_meta,
                "tensor_path": tensor_path
            }
        
        # Parse output for key metrics
        lines = result.stdout.strip().split('\n')
        final_line = [l for l in lines if l.startswith("It.:")][- 1] if any(l.startswith("It.:") for l in lines) else ""
        
        timing_line = [l for l in lines if "End time:" in l]
        total_time = float(timing_line[0].split(":")[1].strip()) if timing_line else None
        
        return {
            "success": True,
            "stdout": result.stdout,
            "final_iteration": final_line,
            "total_time": total_time,
            "grid_size": (nelx, nely, nelz),
            "volume_fraction": vol_frac,
            "input_tensor": input_tensor,
            "load_metadata": load_meta,
            "tensor_path": tensor_path
        }
        
    finally:
        # Clean up temporary load file
        if not keep_load_file and os.path.exists(load_file):
            os.remove(load_file)
            print(f"\nüóëÔ∏è  Cleaned up temporary load file")


def batch_generate(
    num_samples: int,
    nelx: int,
    nely: int,
    nelz: int,
    vol_frac: float = 0.2,
    iters: int = 20,
    output_dir: str = "dataset",
    base_seed: int = 1000
) -> list:
    """
    Generate a batch of paired (input, target) samples for supervised learning.
    
    Creates:
    - sample_{i:04d}_inputs.npy: Input tensor (nelx, nely, nelz, 4)
    - sample_{i:04d}_target.npy: Density field (nelx, nely, nelz)
    - dataset_index.json: Metadata for all samples
    
    Args:
        num_samples: Number of samples to generate
        nelx, nely, nelz: Grid dimensions
        vol_frac: Volume fraction constraint
        iters: Number of design iterations per sample
        output_dir: Output directory for dataset
        base_seed: Base random seed (sample i uses base_seed + i)
    
    Returns:
        list: Metadata for all generated samples
    """
    import json
    import time
    
    # Setup paths
    script_dir = Path(__file__).parent.resolve()
    exe_path = script_dir / "TopOpt-in-OpenMP" / "top3d"
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    if not exe_path.exists():
        raise FileNotFoundError(f"Solver not found at: {exe_path}")
    
    print("=" * 60)
    print(f"ML Dataset Generation: {num_samples} Samples")
    print("=" * 60)
    print(f"Grid: {nelx} √ó {nely} √ó {nelz}")
    print(f"Volume Fraction: {vol_frac}")
    print(f"Iterations: {iters}")
    print(f"Output: {output_path}")
    print("=" * 60)
    
    dataset_index = []
    successful = 0
    
    for i in range(num_samples):
        sample_num = i + 1
        print(f"\n[Sample {sample_num}/{num_samples}]")
        start_time = time.time()
        
        # Create temporary load file
        fd, load_file = tempfile.mkstemp(suffix=".bin", prefix="topopt_load_")
        os.close(fd)
        
        try:
            # Generate randomized load with ML input tensor
            seed = base_seed + i
            _, input_tensor, load_meta = generate_random_load(
                nelx, nely, nelz, load_file, total_force=-1000.0, random_seed=seed
            )
            
            cx, cy = load_meta['load_center']
            print(f"  Load: center=({cx:2d}, {cy:2d}), "
                  f"radius={load_meta['load_radius']:.2f}, "
                  f"nodes={load_meta['num_load_nodes']}")
            
            # Run solver
            nl = 4
            size_incr = 2 ** (nl - 1)
            nelx_coarse = nelx // size_incr
            nely_coarse = nely // size_incr
            nelz_coarse = nelz // size_incr
            
            cmd = [
                str(exe_path),
                "-x", str(nelx_coarse),
                "-y", str(nely_coarse),
                "-z", str(nelz_coarse),
                "-v", str(vol_frac),
                "-i", str(iters),
                "-l", str(nl),
                "-f", load_file
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=str(script_dir / "TopOpt-in-OpenMP")
            )
            
            if result.returncode != 0:
                print(f"  ‚ùå Solver failed!")
                print(result.stderr)
                continue
            
            # Read density field
            density_file = script_dir / "TopOpt-in-OpenMP" / "density.bin"
            if not density_file.exists():
                print(f"  ‚ùå Density file not found!")
                continue
            
            target_density = read_density_field(nelx, nely, nelz, str(density_file))
            
            # Save paired files
            sample_id = f"{sample_num:04d}"
            input_file = output_path / f"sample_{sample_id}_inputs.npy"
            target_file = output_path / f"sample_{sample_id}_target.npy"
            
            np.save(input_file, input_tensor)
            np.save(target_file, target_density)
            
            # Parse solver output for metrics
            timing_line = [l for l in result.stdout.split('\n') if "End time:" in l]
            solve_time = float(timing_line[0].split(":")[1].strip()) if timing_line else None
            
            elapsed = time.time() - start_time
            
            # Save metadata
            metadata = {
                "sample_id": sample_id,
                "grid_size": [nelx, nely, nelz],
                "volume_fraction": vol_frac,
                "iterations": iters,
                "random_seed": seed,
                "load_center": load_meta['load_center'],
                "load_radius": load_meta['load_radius'],
                "num_load_nodes": load_meta['num_load_nodes'],
                "solve_time": solve_time,
                "total_time": elapsed,
                "input_file": str(input_file.name),
                "target_file": str(target_file.name),
                "density_stats": {
                    "min": float(target_density.min()),
                    "max": float(target_density.max()),
                    "mean": float(target_density.mean())
                }
            }
            dataset_index.append(metadata)
            
            successful += 1
            print(f"  ‚úÖ Complete in {elapsed:.1f}s (solve: {solve_time:.2f}s)" if solve_time else f"  ‚úÖ Complete in {elapsed:.1f}s")
            print(f"  Saved: {input_file.name}, {target_file.name}")
            
        finally:
            # Cleanup
            if os.path.exists(load_file):
                os.remove(load_file)
            density_file = script_dir / "TopOpt-in-OpenMP" / "density.bin"
            if density_file.exists():
                density_file.unlink()
    
    # Save dataset index
    index_file = output_path / "dataset_index.json"
    with open(index_file, 'w') as f:
        json.dump(dataset_index, f, indent=2)
    
    # Summary
    print("\n" + "=" * 60)
    print(f"Dataset Generation Complete: {successful}/{num_samples} successful")
    print("=" * 60)
    if successful > 0:
        print(f"\nFiles created:")
        print(f"  Input tensors: {successful} √ó {input_tensor.nbytes / 1024:.1f} KB")
        print(f"  Target densities: {successful} √ó {target_density.nbytes / 1024:.1f} KB")
        print(f"  Total size: {(input_tensor.nbytes + target_density.nbytes) * successful / 1024 / 1024:.2f} MB")
        print(f"\nDataset index: {index_file}")
    
    return dataset_index


# ============================================================================
# Example Usage
# ============================================================================
if __name__ == "__main__":
    print("=" * 60)
    print("TopOpt Data Factory - Paired Dataset Generation")
    print("=" * 60)
    
    # Generate a small batch of paired (X, Y) samples
    dataset_index = batch_generate(
        num_samples=3,
        nelx=64,
        nely=32,
        nelz=32,
        vol_frac=0.2,
        iters=5,
        output_dir="paired_dataset",
        base_seed=42
    )
    
    if len(dataset_index) > 0:
        print("\n" + "=" * 60)
        print("‚úÖ Dataset Generation Successful!")
        print("=" * 60)
        
        # Load and verify first sample
        print("\nVerifying first sample...")
        X = np.load("paired_dataset/sample_0001_inputs.npy")
        Y = np.load("paired_dataset/sample_0001_target.npy")
        
        print(f"  Input tensor (X): {X.shape}, dtype={X.dtype}")
        print(f"  Target density (Y): {Y.shape}, dtype={Y.dtype}")
        print(f"\n  Input channels:")
        print(f"    Ch 0 (solid): sum={X[:,:,:,0].sum():.0f}")
        print(f"    Ch 3 (Fz): sum={X[:,:,:,3].sum():.2f} N")
        print(f"\n Density range: [{Y.min():.4f}, {Y.max():.4f}]")
        print(f"  Density mean: {Y.mean():.4f}")
    else:
        print("\n‚ùå Dataset generation failed!")


